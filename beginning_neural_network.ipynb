{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # reshaping into a one-dimensional tensor.\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(1,28,28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.rand(3,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "hidden1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7813, -0.0109, -0.3009, -0.1772,  0.0075,  0.1917, -0.2727,  0.3089,\n",
       "         -0.3267,  0.2577,  0.1391, -0.3267, -0.2309,  0.1797,  0.1829, -0.2247,\n",
       "         -0.5332,  0.9296, -0.5203, -0.1793],\n",
       "        [ 0.9275, -0.0492, -0.0188, -0.3519, -0.1292,  0.1534, -0.1091, -0.0127,\n",
       "         -0.2173,  0.0867,  0.2852, -0.0727, -0.1840,  0.2838,  0.0091, -0.5561,\n",
       "         -0.4566,  0.7620, -0.4660,  0.1218],\n",
       "        [ 0.5505,  0.0734,  0.1540,  0.0242,  0.0023, -0.0682, -0.4043, -0.2651,\n",
       "         -0.1139,  0.3381,  0.1207, -0.4564, -0.3164,  0.0700, -0.1543, -0.0711,\n",
       "         -0.2271,  0.6964, -0.4854,  0.2805]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7813, 0.0000, 0.0000, 0.0000, 0.0075, 0.1917, 0.0000, 0.3089, 0.0000,\n",
       "         0.2577, 0.1391, 0.0000, 0.0000, 0.1797, 0.1829, 0.0000, 0.0000, 0.9296,\n",
       "         0.0000, 0.0000],\n",
       "        [0.9275, 0.0000, 0.0000, 0.0000, 0.0000, 0.1534, 0.0000, 0.0000, 0.0000,\n",
       "         0.0867, 0.2852, 0.0000, 0.0000, 0.2838, 0.0091, 0.0000, 0.0000, 0.7620,\n",
       "         0.0000, 0.1218],\n",
       "        [0.5505, 0.0734, 0.1540, 0.0242, 0.0023, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.3381, 0.1207, 0.0000, 0.0000, 0.0700, 0.0000, 0.0000, 0.0000, 0.6964,\n",
       "         0.0000, 0.2805]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = nn.ReLU()(hidden1)\n",
    "hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential: 순서를 갖는 모듈의 컨테이너\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10) # input size, output size\n",
    ")\n",
    "\n",
    "input_image = torch.rand(2,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pre_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: linear_relu_stack.0.weight, Size: torch.Size([512, 784]), values: tensor([[ 0.0037,  0.0056,  0.0024,  ..., -0.0332, -0.0268,  0.0115],\n",
      "        [ 0.0223, -0.0281,  0.0275,  ...,  0.0273,  0.0068,  0.0131]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.0.bias, Size: torch.Size([512]), values: tensor([-0.0056, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.2.weight, Size: torch.Size([512, 512]), values: tensor([[-0.0391, -0.0197, -0.0266,  ..., -0.0205,  0.0031, -0.0023],\n",
      "        [-0.0105, -0.0152, -0.0332,  ..., -0.0169, -0.0081, -0.0155]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.2.bias, Size: torch.Size([512]), values: tensor([-0.0437,  0.0375], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.4.weight, Size: torch.Size([10, 512]), values: tensor([[ 0.0357,  0.0194,  0.0359,  ..., -0.0076,  0.0201, -0.0002],\n",
      "        [-0.0323, -0.0054,  0.0035,  ..., -0.0024, -0.0099, -0.0100]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.4.bias, Size: torch.Size([10]), values: tensor([ 0.0219, -0.0345], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Size: {param.size()}, values: {param[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09984021b85b4fc73470db0c076536dd43f155500b22647bf3eda11468cba887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
